{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U \\\n  nest_asyncio \\\n  fastapi \\\n  uvicorn \\\n  pyngrok \\\n  yt-dlp \\\n  moviepy \\\n  langdetect \\\n  requests \\\n  transformers \\\n  torch \\\n  torchvision \\\n  torchaudio \\\n  sentencepiece \\\n  git+https://github.com/openai/whisper.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu117","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/openai/whisper.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"Device:\", torch.cuda.get_device_name(0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Удаляем несовместимые версии\n!pip uninstall -y torch torchvision torchaudio\n\n# Ставим стабильные версии для CUDA 11.7 (P100)\n!pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu117\n\n# Обновляем whisper\n!pip install --upgrade git+https://github.com/openai/whisper.git\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastapi==0.103.2 \"uvicorn[standard]==0.23.2\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"uvicorn[standard]==0.23.2\" fastapi==0.103.2 --force-reinstall\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ngrok authtoken 2ttGu5JNnkTFeQy3QoilPOFwAvV_6hNru66rgjjPERy3aYZb8\nimport nest_asyncio\nnest_asyncio.apply()\n\nimport uvicorn\nfrom pyngrok import ngrok\nfrom fastapi import FastAPI, UploadFile, File, HTTPException, Form\nfrom fastapi.responses import JSONResponse\nimport os, shutil, requests, subprocess, yt_dlp\n\n# ─── Hugging Face Summarizers (CPU only) ───\nfrom transformers import pipeline\nfrom langdetect import detect\n\nsummarizer_en = pipeline(\n    \"summarization\",\n    model=\"facebook/bart-large-cnn\",\n    device=-1   # CPU\n)\nsummarizer_ru = pipeline(\n    \"summarization\",\n    model=\"csebuetnlp/mT5_multilingual_XLSum\",\n    device=-1   # CPU\n)\n\n# ─── Whisper (GPU small) ───\nimport torch, whisper\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device:\", torch.cuda.get_device_name(0))\n\n# грузим модель\nwhisper_model = whisper.load_model(\"small\", device=\"cuda\")\nprint(\"✅ Whisper small GPU загружен\")\n\ndef whisper_transcribe(file_path: str) -> str:\n    result = whisper_model.transcribe(file_path)\n    return result[\"text\"]\n\n\n# ─── Utils ───\nUPLOAD_FOLDER = \"uploads\"\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nMAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB\n\ndef convert_to_wav(input_path: str) -> str:\n    base, _ = os.path.splitext(input_path)\n    output_path = base + \".wav\"\n    subprocess.run(\n        [\"ffmpeg\", \"-y\", \"-i\", input_path, \"-ar\", \"16000\", \"-ac\", \"1\", output_path],\n        check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    return output_path\n\n# 🔥 Фикс: проверяем расширение, чтобы не дергать yt-dlp на mp4/mp3\nALLOWED_DIRECT_EXTS = (\".mp4\", \".mp3\", \".wav\", \".m4a\", \".ogg\", \".mkv\", \".avi\", \".mov\")\n\ndef download_media(source_url: str, output_dir=\"uploads\") -> str:\n    os.makedirs(output_dir, exist_ok=True)\n\n    # если ссылка ведет прямо на файл → качаем requests\n    if any(source_url.lower().endswith(ext) for ext in ALLOWED_DIRECT_EXTS):\n        filename = os.path.basename(source_url.split(\"?\")[0]) or \"downloaded_file\"\n        file_path = os.path.join(output_dir, filename)\n        with requests.get(source_url, stream=True) as r:\n            r.raise_for_status()\n            with open(file_path, \"wb\") as f:\n                shutil.copyfileobj(r.raw, f)\n        return file_path\n\n    # иначе используем yt-dlp\n    output_path = os.path.join(output_dir, \"input.%(ext)s\")\n    ydl_opts = {\n        \"format\": \"bestaudio/best\",\n        \"outtmpl\": output_path,\n        \"quiet\": True,\n        \"noplaylist\": True,\n    }\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(source_url, download=True)\n        return ydl.prepare_filename(info)\n\n# ─── Chunk helper ───\ndef chunk_text(text, max_len=800):\n    words = text.split()\n    chunks, cur = [], []\n    for w in words:\n        cur.append(w)\n        if len(\" \".join(cur)) > max_len:\n            chunks.append(\" \".join(cur))\n            cur = []\n    if cur:\n        chunks.append(\" \".join(cur))\n    return chunks\n\ndef generate_summary(text: str) -> str:\n    try:\n        lang = detect(text)\n    except:\n        lang = \"en\"\n\n    summarizer = summarizer_ru if lang == \"ru\" else summarizer_en\n\n    chunks = chunk_text(text, max_len=800)\n    summary_parts = []\n    for ch in chunks:\n        out = summarizer(ch, max_length=200, min_length=50, do_sample=False)\n        summary_parts.append(out[0][\"summary_text\"])\n    return \" \".join(summary_parts)\n\n# ─── FastAPI ───\napp = FastAPI()\n\n@app.post(\"/transcribe\")\nasync def transcribe(\n    file: UploadFile = File(None),\n    source_url: str = Form(None),\n    do_summary: bool = Form(True),\n    session_id: str = Form(\"default\")\n):\n    session_folder = os.path.join(UPLOAD_FOLDER, session_id)\n    os.makedirs(session_folder, exist_ok=True)\n\n    if source_url:\n        try:\n            file_path = download_media(source_url, session_folder)\n        except Exception as e:\n            raise HTTPException(status_code=400, detail=f\"Ошибка при скачивании: {e}\")\n    elif file:\n        file.file.seek(0, 2)\n        size = file.file.tell()\n        file.file.seek(0)\n        if size > MAX_FILE_SIZE:\n            raise HTTPException(status_code=400, detail=f\"Файл слишком большой: {size} байт\")\n        file_path = os.path.join(session_folder, file.filename)\n        with open(file_path, \"wb\") as buffer:\n            shutil.copyfileobj(file.file, buffer)\n    else:\n        raise HTTPException(status_code=400, detail=\"Нужно прислать файл или ссылку\")\n\n    try:\n        wav_path = convert_to_wav(file_path)\n        text = whisper_transcribe(wav_path)\n\n        summary = None\n        if do_summary and text.strip():\n            summary = generate_summary(text)\n\n        return {\"text\": text, \"summary\": summary, \"session_id\": session_id}\n    except Exception as e:\n        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n\n# ─── Ngrok ───\npublic_url = ngrok.connect(8000)\nprint(\"🔗 Public URL для Telegram-бота:\", public_url)\n\nuvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T02:05:15.330996Z","iopub.execute_input":"2025-09-20T02:05:15.331726Z","iopub.status.idle":"2025-09-20T02:33:30.279548Z","shell.execute_reply.started":"2025-09-20T02:05:15.331692Z","shell.execute_reply":"2025-09-20T02:33:30.278685Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"CUDA available: True\nDevice: Tesla T4\n✅ Whisper small GPU загружен\n🔗 Public URL для Telegram-бота: NgrokTunnel: \"https://5161fd3f32a5.ngrok-free.app\" -> \"http://localhost:8000\"\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [259]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nYour max_length is set to 200, but your input_length is only 172. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\nYour max_length is set to 200, but your input_length is only 181. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=90)\nYour max_length is set to 200, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n","output_type":"stream"},{"name":"stdout","text":"INFO:     95.82.70.6:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [259]\n","output_type":"stream"}],"execution_count":2}]}